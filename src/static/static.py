"""
This module contains the functions to extract features from a Portable Executable file
and predict the class of the file using a machine learning model.

"""

import pefile
import joblib
import numpy as np
import pandas as pd
from capstone import *
from sklearn.preprocessing import Normalizer

model_version = 'v2'

def extract_features(path):

    """
    Extracts features from a Portable Executable file using the pefile library.
    It extracts 42 features from the PE file.

    Parameters:
    - path (str): Path to the Portable Executable file.
 
    """

    # Check if the file is a valid PE file
    try:
        sample = pefile.PE(path)
    except Exception as e:
        return None

    # Define the features dictionary and add the name of the file
    features = {}
    features['Name'] = path.split('\\')[-1]

    # Define the features to extract from the PE file
    attributes = [
        'OPTIONAL_HEADER.CheckSum', 'FILE_HEADER.Machine', 'FILE_HEADER.SizeOfOptionalHeader',
        'FILE_HEADER.Characteristics', 'OPTIONAL_HEADER.MajorLinkerVersion', 'OPTIONAL_HEADER.MinorLinkerVersion',
        'OPTIONAL_HEADER.SizeOfCode', 'OPTIONAL_HEADER.SizeOfInitializedData', 'OPTIONAL_HEADER.SizeOfUninitializedData',
        'OPTIONAL_HEADER.AddressOfEntryPoint', 'OPTIONAL_HEADER.BaseOfCode', 'OPTIONAL_HEADER.BaseOfData',
        'OPTIONAL_HEADER.ImageBase', 'OPTIONAL_HEADER.SectionAlignment', 'OPTIONAL_HEADER.FileAlignment',
        'OPTIONAL_HEADER.MajorOperatingSystemVersion', 'OPTIONAL_HEADER.MinorOperatingSystemVersion',
        'OPTIONAL_HEADER.MajorImageVersion', 'OPTIONAL_HEADER.MinorImageVersion', 'OPTIONAL_HEADER.MajorSubsystemVersion',
        'OPTIONAL_HEADER.MinorSubsystemVersion', 'OPTIONAL_HEADER.SizeOfImage', 'OPTIONAL_HEADER.SizeOfHeaders',
        'OPTIONAL_HEADER.CheckSum', 'OPTIONAL_HEADER.Subsystem', 'OPTIONAL_HEADER.DllCharacteristics',
        'OPTIONAL_HEADER.SizeOfStackReserve', 'OPTIONAL_HEADER.SizeOfStackCommit', 'OPTIONAL_HEADER.SizeOfHeapReserve',
        'OPTIONAL_HEADER.SizeOfHeapCommit', 'OPTIONAL_HEADER.LoaderFlags', 'OPTIONAL_HEADER.NumberOfRvaAndSizes',
        'FILE_HEADER.NumberOfSections'
    ]

    # Add the features to the dictionary
    for attr in attributes:
        parts = attr.split('.')
        obj = sample
        for part in parts:
            if hasattr(obj, part):
                obj = getattr(obj, part)
            else:
                obj = None
                break
        if obj is not None:
            features[attr] = obj
        else:
            features[attr] = 0

    # Extract the entropy, raw size and virtual size of the sections
    MeanEntropy, MaxEntropy, MinEntropy = [], [], []
    MeanRawsize, MaxRawsize, MinRawsize = [], [], []
    MeanVirtualsize, MaxVirtualsize, MinVirtualsize = [], [], []

    for section in sample.sections:
        MeanEntropy.append(section.get_entropy())
        MeanRawsize.append(section.SizeOfRawData)
        MeanVirtualsize.append(section.Misc_VirtualSize)
        MaxEntropy.append(section.get_entropy())
        MaxRawsize.append(section.SizeOfRawData)
        MaxVirtualsize.append(section.Misc_VirtualSize)
        MinEntropy.append(section.get_entropy())
        MinRawsize.append(section.SizeOfRawData)
        MinVirtualsize.append(section.Misc_VirtualSize)

    # Calculate the mean, max and min of the entropy, raw size and virtual size
    try:
        features['MeanEntropy'] = sum(MeanEntropy) / float(len(MeanEntropy))
        features['MaxEntropy'] = max(MaxEntropy)
        features['MinEntropy'] = min(MinEntropy)
    except:
        features['MeanEntropy'] = 0
        features['MaxEntropy'] = 0
        features['MinEntropy'] = 0

    # Calculate the mean, max and min of the raw size
    try:
        features['MeanRawsize'] = sum(MeanRawsize) / float(len(MeanRawsize))
        features['MaxRawsize'] = max(MaxRawsize)
        features['MinRawsize'] = min(MinRawsize)
    except:
        features['MeanRawsize'] = 0
        features['MaxRawsize'] = 0
        features['MinRawsize'] = 0

    # Calculate the mean, max and min of the virtual size
    try:
        features['MeanVirtualsize'] = sum(MeanVirtualsize) / float(len(MeanVirtualsize))
        features['MaxVirtualsize'] = max(MaxVirtualsize)
        features['MinVirtualsize'] = min(MinVirtualsize)
    except:
        features['MeanVirtualsize'] = 0
        features['MaxVirtualsize'] = 0
        features['MinVirtualsize'] = 0

    # Extract the number of imports and the number of DLLs
    try:
        # Extract number of DLLs imported from the PE file
        features['ImportsNbDLL'] = len(sample.DIRECTORY_ENTRY_IMPORT)
        
        # Extract number of imports from each DLL
        import_count = 0
        for entry in sample.DIRECTORY_ENTRY_IMPORT:
            import_count += len(entry.imports)

        features['ImportsNb'] = import_count

        # Extracting the imports names
        imports = []
        for entry in sample.DIRECTORY_ENTRY_IMPORT:
            for imp in entry.imports:
                imports.append(imp.name.decode('utf-8'))

        # Add the imports to the dictionary
        features['DLLImports'] = ' '.join(imports)
    except:
        features['ImportsNbDLL'] = 0
        features['ImportsNb'] = 0
        features['DLLImports'] = ''

    # Extracting the opcodes from the code sections
    try:
        opcodes = []

        # Locate the code sections and extract the opcodes
        for section in sample.sections:
            if section.Characteristics & 0x20:
                code = section.get_data()

                # Decode the code and print the disassembled code
                disassembler = Cs(CS_ARCH_X86, CS_MODE_32)
                for instruction in disassembler.disasm(code, section.VirtualAddress):
                    opcodes.append(instruction.mnemonic)

        # Add the opcodes to the dictionary
        features['Opcodes'] = ' '.join(opcodes)
    except:
        features['Opcodes'] = ''

    # Stop the pefile module
    sample.close()

    return features



def preprocess_data(data):

    """
    Preprocesses the data by cleaning it and preparing it for the machine learning model.

    Parameters:
    - data (dict): Dictionary containing the features extracted from the Portable Executable file.

    """

    # Convert the data to a pandas dataframe
    data = pd.DataFrame([data])

    # Drop duplicates
    df = data.drop_duplicates()
    
    # Reset index
    df = df.reset_index(drop=True)

    # Remove non numerical columns
    data1 = df.drop(['Name', 'Opcodes', 'DLLImports'], axis=1)

    # Keep non-numeric columns for processing
    data2 = df[['Opcodes', 'DLLImports']]

    # Replace nan with empty string
    data2 = data2.fillna('')

    # Load the tf-idf vectorizers
    vectorizer_dllimports = joblib.load(f'./src/static/models/{model_version}/vectorizer_dllimports.pkl')
    vectorizer_opcodes = joblib.load(f'./src/static/models/{model_version}/vectorizer_opcodes.pkl')

    # Transform the text data using the tf-idf vectorizers
    dllimportsTFIDF = vectorizer_dllimports.transform(data2['DLLImports'])
    opcodesTFIDF = vectorizer_opcodes.transform(data2['Opcodes'])

    # Combine the two vectors into one
    combinedTFIDF = np.concatenate([dllimportsTFIDF.toarray(), opcodesTFIDF.toarray()], axis=1)

    # Normalize the combined vectors to have unit norm
    normalizer = Normalizer()
    combinedTFIDF = normalizer.fit_transform(combinedTFIDF)

    # Create a dataframe from the combined vectors to save to csv
    features = vectorizer_dllimports.get_feature_names_out().tolist() + vectorizer_opcodes.get_feature_names_out().tolist()
    data2 = pd.DataFrame(combinedTFIDF, columns=features)

    # Combine the converted text data with the numerical data
    data = pd.concat([data1, data2], axis=1)
    data.reset_index(drop=True, inplace=True)

    # Feature selection
    features = joblib.load(f'./src/static/models/{model_version}/important_features.pkl')

    # Select only the important features
    data = data[features]

    # Load the scaler
    scaler = joblib.load(f'./src/static/models/{model_version}/scaler.pkl')

    # Remove the feature names
    data = data.values

    # Scale the data
    data = scaler.transform(data)

    return data


def predict(data):
    
    """
    Predicts the class of the data using the machine learning model.

    Parameters:
    - data (dict): Dictionary containing the features extracted from the Portable Executable file.

    """

    # convert the data to a pandas dataframe
    data = pd.DataFrame(data)

    # load the machine learning model
    model = joblib.load(f'./src/static/models/{model_version}/random_forest_model.pkl')

    # predict the class of the data
    prediction = model.predict(data)[0]
    confidence = model.predict_proba(data)

    return {'prediction': prediction, 'confidence': round(max(confidence[0]) * 100, 2)}
