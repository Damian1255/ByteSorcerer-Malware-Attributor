import joblib
import json

import numpy as np
import pandas as pd
from sklearn.utils import resample
from sklearn.preprocessing import Normalizer
from sklearn.feature_extraction.text import TfidfVectorizer

def preprocess_data(data):

    """
    Preprocesses the data by cleaning it and preparing it for the machine learning model.

    """

    # convert the data to a pandas dataframe
    data = pd.DataFrame([data])

    # drop duplicates
    df = data.drop_duplicates()
    
    # reset index
    df = df.reset_index(drop=True)

    # remove non numerical columns
    data1 = df.drop(['Name', 'Opcodes', 'DLLImports'], axis=1)

    # keep non-numeric columns for processing
    data2 = df[['Opcodes', 'DLLImports']]

    # replace nan with empty string
    data2 = data2.fillna('')

    # load the tf-idf vectorizers
    vectorizer_dllimports = joblib.load('./src/models/vectorizer_dllimports.pkl')
    vectorizer_opcodes = joblib.load('./src/models/vectorizer_opcodes.pkl')

    # transform the text data using the tf-idf vectorizers
    dllimportsTFIDF = vectorizer_dllimports.transform(data2['DLLImports'])
    opcodesTFIDF = vectorizer_opcodes.transform(data2['Opcodes'])

    # combine the two vectors into one
    combinedTFIDF = np.concatenate([dllimportsTFIDF.toarray(), opcodesTFIDF.toarray()], axis=1)

    # normalize the combined vectors to have unit norm
    normalizer = Normalizer()
    combinedTFIDF = normalizer.fit_transform(combinedTFIDF)

    # create a dataframe from the combined vectors to save to csv
    features = vectorizer_dllimports.get_feature_names_out().tolist() + vectorizer_opcodes.get_feature_names_out().tolist()
    data2 = pd.DataFrame(combinedTFIDF, columns=features)

    # combine the converted text data with the numerical data
    data = pd.concat([data1, data2], axis=1)
    data.reset_index(drop=True, inplace=True)

    # feature selection
    features = joblib.load('./src/models/important_features.pkl')

    # select only the important features
    data = data[features]

    # load the scaler
    scaler = joblib.load('./src/models/scaler.pkl')

    # scale the data
    data = scaler.transform(data)

    return data


def predict(data):
    
    """
    Predicts the class of the data using the machine learning model.

    """

    # convert the data to a pandas dataframe
    data = pd.DataFrame(data)

    # load the machine learning model
    model = joblib.load('./src/models/random_forest_model.pkl')

    # predict the class of the data
    prediction = model.predict(data)[0]
    confidence = model.predict_proba(data)

    return {'prediction': prediction, 'confidence': round(max(confidence[0]) * 100, 2)}


def analyze_traits(sample_traits, n=100):
    # find the top n common traits in the sample
    sample_traits = pd.DataFrame(sample_traits, columns=['trait', 'family'])
    sample_traits = sample_traits['trait'].value_counts().head(n)
    sample_traits = pd.DataFrame(sample_traits)
    sample_traits.columns = ['count']
    sample_traits['trait'] = sample_traits.index
    sample_traits = sample_traits.reset_index(drop=True)
    sample_traits = sample_traits[['trait', 'count']]

    # load the top n traits
    top_n_traits = pd.read_csv('./src/top_n_traits.csv')

    # find the common traits in the sample
    results = {}
    for family in top_n_traits['family'].unique():
        c = 0
        for x in sample_traits['trait']:
            if x in top_n_traits[top_n_traits['family'] == family]['trait'].tolist():
                c += 1

        results[family] = c

    results = pd.DataFrame(results.items(), columns=['family', 'count'])
    results = results.sort_values(by='count', ascending=False)
    results = results.reset_index(drop=True)
    
    print(results)

    return {'prediction': results['family'][0], 'confidence': 100}